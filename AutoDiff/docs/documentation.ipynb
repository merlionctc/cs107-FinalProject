{"cells":[{"cell_type":"code","metadata":{"output_cleared":false,"source_hash":"9e5190f0","execution_millis":135,"cell_id":"00000-b743eac2-5bbf-4003-b687-9aa1017b5594","execution_start":1607564854722,"deepnote_cell_type":"code"},"source":"# RUN THIS CELL FOR FORMAT\nimport requests\nfrom IPython.core.display import HTML\nstyles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\nHTML(styles)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\nblockquote { background: #AEDE94; }\nh1 { \n    padding-top: 25px;\n    padding-bottom: 25px;\n    text-align: left; \n    padding-left: 10px;\n    background-color: #DDDDDD; \n    color: black;\n}\nh2 { \n    padding-top: 10px;\n    padding-bottom: 10px;\n    text-align: left; \n    padding-left: 5px;\n    background-color: #EEEEEE; \n    color: black;\n}\n\ndiv.exercise {\n\tbackground-color: #ffcccc;\n\tborder-color: #E9967A; \t\n\tborder-left: 5px solid #800080; \n\tpadding: 0.5em;\n}\n\ndiv.exercise-r {\n\tbackground-color: #fce8e8;\n\tborder-color: #E9967A; \t\n\tborder-left: 5px solid #800080; \n\tpadding: 0.5em;\n}\n\n\nspan.sub-q {\n\tfont-weight: bold;\n}\ndiv.theme {\n\tbackground-color: #DDDDDD;\n\tborder-color: #E9967A; \t\n\tborder-left: 5px solid #800080; \n\tpadding: 0.5em;\n\tfont-size: 18pt;\n}\ndiv.gc { \n\tbackground-color: #AEDE94;\n\tborder-color: #E9967A; \t \n\tborder-left: 5px solid #800080; \n\tpadding: 0.5em;\n\tfont-size: 12pt;\n}\np.q1 { \n    padding-top: 5px;\n    padding-bottom: 5px;\n    text-align: left; \n    padding-left: 5px;\n    background-color: #EEEEEE; \n    color: black;\n}\nheader {\n   padding-top: 35px;\n    padding-bottom: 35px;\n    text-align: left; \n    padding-left: 10px;\n    background-color: #DDDDDD; \n    color: black;\n}\n</style>\n\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Documentation\n@merlionctc\n\n**Table of Contents**\n- [1. Introduction](#1.-Introduction)\n  - [1.1 Derivative](#1.1-Derivatives)\n  - [1.2 Automatic Differentiation](#1.2-Auto-Differentiation)\n- [2. Background](#2.-Background)\n  - [2.1 Chain Rule](#2.1-Chain-Rule) \n  - [2.2 Elementary functions](#2.2-Elementary-functions) \n  - [2.3 Forward mode](#2.3-Forward-mode) \n  - [2.4 Dual Number](#2.4-Dual-Number) \n  - [2.5 Reverse Mode](#2.5-Reverse-Mode) \n- [3. Usage](#3.-Usage)\n  - [3.1 Installation](#3.1-Installation)\n  - [3.2 How to use](#3.2-How-to-Use)\n- [4. Software organization](#4.-Software-Organization)\n  - [4.1 Directory Structure](#4.1-Directory-Structure)\n  - [4.2 Modules to Include](#4.2-Modules-to-Include)\n  - [4.3 Test Suite Design](#4.3-Test-Suite-Design)\n  - [4.4 Package Distribution](#4.4-Package-Distribution) \n- [5. Implementation](#5.-Implementation)\n  - [5.1 Forward Mode Implementation](#5.1-Forward-Mode-Implementation)\n  - [5.2 Symbolic Reverse Mode](#5.2-Symbolic-Reverse-Mode)\n  - [5.3 Handling Elementary Functions](#5.2-Handling-Elementary-Functions)\n- [6. Extensions](#6.-Extensions)\n  - [6.1 Future Features from Milestone2 with updates](#6.1-Future-Features-from-Milestone2-with-updates)\n  - [6.2 Revere Mode](#6.2-Revere-Mode)\n  - [6.3 Symbolic Expression (Reverse Diff and Higher Order Derivative)](#6.3-Symbolic-Expression-(Reverse-Diff-and-Higher-Order-Derivative))\n- [7. Broader Impact and Inclusivity Statement](#7.-Broader-Impact-and-Inclusivity-Statement)\n- [8. Futures](#8.-Futures)\n- [9. Reference](#9.-Reference)\n\n\n\n\n\n## 1. Introduction\n\nWe developed this package, `AutoDiff`,  in the light of automatic differentiation. The package can help to automatically differentiate a function input into the program. The package includes modules of forward-mode differentiation and backward-mode differentiation.\n\n### 1.1 Derivatives\n\nFormally, for single variable case, the derivative of a function, if it exists, is defined as\n\n$$ f'(x) = \\lim_{h\\to0} \\frac{f(a+h) - f(a)}{h} $$\n\nto visualize, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value.\n\nOf course, derivatives may be generalized to functions of several real variables, and derivatives are useful in finding the maxima and minima of functions. Derivatives has a variety of applications in statistics and machine learning, and the process of finding a derivative is called *differentiation*.\n\nThere are three ways of differentiation realized in computer science:\n- Numerical differentiation\n- Symbolic differentiation\n- Automatic differentiaton\n\n\n### 1.2 Auto Differentiation\nAutomatic differentiation (AD), also called as algorithmic differentiation, is a set of techniques for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs <sup>1</sup>. It is not numerical differentiation, since numerical differentiation is the finite difference approximation of derivatives using the values of the original function evaluated at some sample points<sup>2</sup>. It is different from symbolic differentiation, since symbolic differentiation is the automatic manipulation of expressions for obtaining derivative expression<sup>3</sup>.\n\nThe essence of AD is that all numerical computations are ultimately compositions of a finite set of elementary operations for which the derivatives are easily known<sup>4</sup>. The algorithm of AD breaks down a function by looking at the sequence of elementary arithmetic operations (addition, subtraction, multiplication and division) and elementary functions (exponential, logrithmatic, and trigonometry). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to machine accuracy.\n\nThis differentiation technique well-established and used with applications in different areas such as fluid dynamics, astronomy, and engineering design optimization.\n\nTo sum up, there are two major advantages of using AD:\n- Computes derivatives to machine precision.\n- Does not rely on extensive mathematical derivations or expression trees, so it is easily applicable to a wide class of functions.\n\n## 2. Background\n\nAutomatic differentiation relies on several vital mathematical foundations, some of which will be illustrated at this part. Based on these conceptions, it will be more resonable for users to understand the software.\n\n### 2.1 Chain Rule\n\nChain Rule is the most important concept in AD. It enables us to deal with complex functions with several layers and arguments. With implementing chain rule, we can easily divide the original complicated functions into basic parts made up with elementary functions, of which we will know the concrete derivative expressions.\n\nSuppose there is a function $h\\left(u\\left(t\\right)\\right)$ and in order to calculate derivative of $h$ with respect to $t$, we should use chain rule. The derivative is $$\\dfrac{\\partial h}{\\partial t} = \\dfrac{\\partial h}{\\partial u}\\dfrac{\\partial u}{\\partial t}.$$\n\nIn general, if a function $h$ has several arguments, or even its argument is a vector, so that $h = h(x(t))$ where  $x \\in R^n$ and $t \\in R^m $. In this way, $h$ is now the combination of $n$ functions, each of which has $m$ variables. The derivative of $h$ is now\n\n $$\\nabla_{t}h = \\sum_{i=1}^{n}{\\frac{\\partial h}{\\partial x_{i}}\\nabla y_{i}\\left(t\\right)}.$$\n\n### 2.2 Elementary functions\n\nAny complex function is made up with several elementary functions. As discussed above, we use chain rule to break it down and then focus on elementary functions to calulate their derivatives. \n\nIn mathematics, an elementary function is a function of a single variable composed of particular simple functions. Elementary functions are typically defined as a sum, product and/or composition of many polynomials, rational functions, trigonometric and exponential functions, and their inverse functions.<sup>5</sup>\n\nOn the other hand, we know the concrete mathematical expression of the elementary functions, which will be used directly in the later graph structure of calculations. \n\n\n### 2.3 Forward mode\n\n#### 2.3.1 Evaluation trace\n\nTake the example of $g = (x+y)*z$, we will first demonstrate the evaluation trace and then its corresponding computational graph.\n\nLet's evaluate g at the point (1,1,1). In the evaluation trace table, we will record the trace of each step, its  elementary operation as well as the corresponding numeric value at the point.\n\n| Trace | Elementary Operation | Numeric Value |\n| ----- | -------------------- | ------------- |\n| $x$   | 1                    | 1             |\n| $y$   | 1                    | 1             |\n| $z$   | 1                    | 1             |\n| $p$   | $x+y$                | 2             |\n| $f$   | $v_1*z$              | 2             |\n\n#### 2.3.2 Computational graph\n\nThe above evaluation trace can be easily visualized with the computational graph below. The node will represent the trace and the edge will represent the elementary operation.\n\n<img src=\"forward_mode.png\">\n\n*Note: If you cannot see the image, please right click and open image in new tab*\n\n#### 2.3.3 Explanation\nThe evaluation trace above is just the path we will follow in forward mode. On top of that, we will also carry the derivatives. And we will take the derivative of g on x.\n\n| Trace | Elementary Operation | Numeric Value | Deri. on x    | Deri. Value on x |\n| ----- | -------------------- | ------------- | ------------- | ---------------- |\n| $x$   | 1                    | 1             | 1             | 1                |\n| $y$   | 1                    | 1             | 0             | 0                |\n| $z$   | 1                    | 1             | 0             | 0                |\n| $v_1$ | $x+y$                | 2             | $\\dot{x}$     | 1                |\n| $f$   | $v_1*z$              | 2             | $\\dot{v_1}*z$ | 1                |\n\n ### 2.4 Dual Number\n \n A dual number has a real part and a dual part. Say we have $z = a + b\\epsilon $. Then $a$ is the real part and $b$ is the dual part.\n\n For $\\epsilon$, we define $\\epsilon ^2 = 0$ but $\\epsilon$ is not zero.\n \n Dual Number is really useful when we want to calculate derivatives of a function. For example, say we have\n\n $$ x = a + b\\epsilon,   y = x^2$$\n \n Then we can derive\n \n $$y = (a + b\\epsilon)^2 = a^2 + 2ab\\epsilon + b^2\\epsilon^2 = a^2 + 2ab\\epsilon$$\n\n Therefore, it is really convenient to get the value of y from real part and get derivative of y from dual part. This is what we will implement in our FORWARD auto differentiation code.\n \n \n### 2.5 Reverse Mode\n\nRecall from the background section, we know that reverse mode is another way of autodifferentation and below is the math background.\n\n#### 2.5.1 Explanation and Background\n\nIt should be noticed that in forward mode, chain rule is not utilized. We just follow the evaluation trace and combine the derivatives of elementary functions together. But for reverse mode, we will implement chain rule. \n\nHowever, it is important to realize that the reverse mode also requires the evaluation trace on forward mode to have the derivatives on the elementary functions. Then we will use chain rule to reversely calculate the final derivative.\n\nThe steps we use to implement reverse mode based on the evaluation trace in forward mode is as follows.\n\n- STEP1: Start with $v_1$\n\n   $$\\overline{v_1} = \\dfrac{\\partial f}{\\partial v_1} = 1.$$\n\n- STEP2: Use chain rule to calculate$\\overline{x}$\n\n   $$\\overline{x} = \\dfrac{\\partial f}{\\partial v_1}\\dfrac{\\partial v_1}{\\partial x}  = 1.$$\n\n- STEP3: Get the derivative on x\n\n   $$\\overline{x} = \\dfrac{\\partial f}{\\partial x}  = 1.$$\n\n** Computational graph **\n\nIn reverse mode, we just reversely implement chain rule to get the derivatives. And the computational graph for reverse mode is as follows.\n\n<img src=\"reverse_mode.png\">\n\n*Note: If you cannot see the image, please right click and open image in new tab*\n\n\n","metadata":{"output_cleared":false,"tags":[],"cell_id":"00001-38382959-638d-48ce-a914-b41306f3057c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 3. Usage\n\n\n ### 3.1 Installation\n\nThere are two ways to install the `AutoDiff` package. \n\nMethod 1 directly install the package through pip into your local environment.\n\nMethod 2 create a virtual environment and install the package through pip, and also clones github repo for `demo.py`.\n\nThe package is distributed through TestPyPI at https://test.pypi.org/project/autodiff-merlionctc/\n\n\n* **Method 1: install packaged using pip** \n\n    To install AutoDiff directly using pip, in the terminal, type:\n\n    ```bash\n    pip install -i https://test.pypi.org/simple/ autodiff-merlionctc\n    ```\n\n    This will install all required modules as dependency.\n\n    Next, user could start use AutoDiff package by following `demo.py` examples in github repository or **3.2 How to Use** section:\n    \n    ```python\n    >>> from autodiff.model import *\n    >>> from autodiff.dual import *\n    >>> from autodiff.elementary import *\n    >>> from autodiff.symbolic import *\n    #（begin autodifferentiation)\n    >>> quit()\n    ```\n\n\n* **Method 2: install using virtual environment**\n    \n    For now, to get started, please do git clone on our project and test by using `demo.py`\n\n    First, download the package from github to your folder\n\n    ```bash\n    mkdir test_merlionctc\n    cd test_merlionctc\n    git clone https://github.com/merlionctc/cs107-FinalProject.git\n    cd cs107-FinalProject\n    ```\n\n    Create a vertual environment and activate it\n\n    ```bash\n    # If you do not have virtualenv, install it\n    sudo easy_install virtualenv\n    # Create virtual environment\n    virtualenv ac207\n    # Activate your virtual environment\n    source ac207/bin/activate\n    ```\n\n    To ensure you have your enviroment and all required package setup, \n\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n    To install our package while using \n    \n    ```bash\n    pip install -i https://test.pypi.org/simple/ autodiff-merlionctc\n    ```\n\n    To run a demo we provided.\n\n    ```bash\n    python3 AutoDiff/demo.py\n    ```\n    \n    If you want to quit the virtual enviornment:\n\n    ```bash\n    deactivate\n    ```\n    \n\n### 3.2 How to Use\n\nHere is an example that serves that a quick start tutorial on Forward Mode.\n\n** Disclaimer: For usage of reverse mode and symbolic, please refer to Section 6 Extension. **\n\nAfter installing AutoDiff package (see section 3.1)\n\n```python\n>>> from autodiff.dual import *\n>>> from autodiff.elementary import *\n>>> from autodiff.model import *\n>>> from autodiff.symbolic import *\n>>> import numpy as np\n```\n\nFirst Step: User instantiate variables\n* val: value of variable that you start with\n* der: value of the derivative of variable that you start with, usually starting with 1\n* loc: The location/index this variable when there are multiple input variables for the target function(s). For example, if you initialize x1 first, the loc will be 0; then you initialize y1, the loc will increment to 1\n* length: The length/number of the total variables that will be input when there are multiple input variables for the target function(s).For example, if you want to initialize x1,y1 and z1, the length will be 3, for each variable in the initialization process\n\n```python\n>>>x1 = Dual(val = 1, der=1, loc = 0, length = 3)\n>>>y1 = Dual(val = 2, der=1, loc = 1, length = 3)\n>>>z1 = Dual(val = 5, der=1, loc = 2, length = 3)\n```\n\nSecond Step: User inputs function, based on above variables\n```python\n>>>f1 = 3 * x1 + 4 * y1 * 2 - cos(z1)\n```\n\nThird Step: User instantiate `autodiff.Forward` class \n```python\n>>>fwd_test = Forward(f1)\n```\n\nFourth Step: User could choose to call instance method `get_value()` to get value of func\n```python\n>>>print(fwd_test.get_value())\n18.716337814536775\n```\n\nFifth Step: User could choose to call instance method `get_der()` to get derivatives of func\n\nNote: This method will return a derivative vector w.r.t to ALL variables. \n\n\nNote 2: If user enters a scalar function, then get_der will return the jacobian\n```python\n>>>print(fwd_test.get_der())\n[ 3.          8.         -0.95892427]\n```\n\nSixth Step: User could choose to call instance method get_der(var) to get derivatives of func\n\nNote: This method will return a derivative vector w.r.t to specific variables you input\n\n```python\n>>>print(fwd_test.get_der(x1))\n[3.0]\n```\n\nSeventh Step: User could also inputs multiple functions with multiple variables and call get_der() and get_jacobian(). \n\n```python\ny1 = Dual(val = np.pi, der=1, loc = 1, length = 3)\nf2 = (tanh(cos(sin(y1))**z1) + logistic(z1**z1, 2, 3, 4))**(1/x1)\nf3 = exp(arccos(tan(sin(y1))) + logb(z1**(1/2), 1/5)*sinh(x1))\n\n# User should use list to combine multiple functions together\nfwd_test_multiple = Forward([f1, f2, f3])\n\n# User could choose single/several variables to get derivatives\nprint(fwd_test_multiple.get_der(x1, y1))\n\n# User could get the jacobian matrix of multiple functions\n# Note: the order displayed in the Jacobian Matrix is matched with the order of input functions(as row) and the input variables(as column)\nprint(fwd_test_multiple.get_jacobian())\n```\n\nquitting\n```python\n>>>quit()\n```\n\nNote: Our example is a generalization of vector input, since if x,y, are vectors, we can decompose x, y into \\[x0 x1] and \\[y0 y1].\nWhen we want to evaluate f = ax + by, we will achieve the same goal of having a list of multiple functions \\[f1 f2], where f1 = a\\*x0+b*y0, f2 = a\\*x1+b\\*y0 after proper linear algebra process.\n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00004-dae35070-7695-4cdd-9719-7ceb3858b634","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 4. Software Organization\n\nDiscuss how you plan on organizing your software package. We provided a basic structure here. \n\n### 4.1 Directory Structure\n\n```\nAutoDiff\n│   README.md\n│   .travis.yml  \n│   .coverage\n│   requirements.txt\n│\n└───src\n│   │\n│   └─── autodiff\n│   │   │   \n│   │   └─── symbolic\n│   │   │   │  __init__.py (wrapper function to use Expression class)\n│   │   │   │  expression.py (symbolic differentiation)\n│   │   │   \n│   │   │   __init__.py\n│   │   │   model.py (AutoDiff main class with Forward mode)\n│   │   │   elementary.py (Elementary Function module)\n│   │   │   dual.py (Dual Number Class for Forward Class)\n│   │\n│   └─── deprecated (some deprecated implementation of reverse)\n│\n└───tests\n│   │   test_autodiff.py\n│   │   test_dual_class.py\n│   │   test_elementary.py\n│   │   test_symbolic.py\n│   │   ...\n└───Docs\n│   │   README.md\n│   │   milestone1.ipynb\n│   │   milestone2_progress.ipynb\n│   │   milestone2.ipynb\n│   │   documentation.ipynb\n│   │   ...\n│     \n│   \n└───demo.py (A demo to usage of package)\n\n```\n\n### 4.2 Modules to Include\n\n Standrad Python libraries/modules:\n\n *math*: mathematical, algebric operations\n\n *numpy*: supports computations for large, multi-dimensional arrays and matrices. \n\n Self-designed libraries/modules under `AutoDiff` package:\n\n *elementary*: contains overwritten elementary functions customized for the different variable class defined when implementing different types of differentiation.\n\n *model*: An interface-like, main class with Forward/Reverse mode auto-differentation methods. Also, the methods are useable for symbolic differentiation \n\n *dual*: This module contains class methods for dual number class, which is the basic class structure in the forward mode\n\n *expression*: This module contains class methods and child classes of expression class, which is the basic class structure of symbolic differentiation\n\n\n### 4.3 Test Suite Design\n\n We used Pytest as our test suite. We tested all of our implemented classes on every single instance methods.\n We also tested on special and corner cases such as simplication, real number etc. \n The whole test suite is included in the *tests* sub-directory. \n And both TravisCI and CodeCov will be used to check the codes coverage and test integration.\n\n### 4.4 Package Distribution\n\n  * Package Distribution\n\n    We distributed our software using [Test Python Package Index (TestPyPi)](https://test.pypi.org/).\n    Currently, we used Python project structure from [PyScaffold](https://pypi.org/project/PyScaffold/).\n    \n    The package could be found here: https://test.pypi.org/project/autodiff-merlionctc/\n\n  * Software package\n\n    We packaged our module using the standard packaging tool ([setuptools](https://packaging.python.org/key_projects/#setuptools)), and following Package Python Projects [Tutorial](https://packaging.python.org/tutorials/packaging-projects/).\n  As mentioned in 3.1 **_How to Install_**, the user can use `pip install` to install the necessary dependencies.\n\n  \n\n\n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00005-85d314d9-e4fb-4bc4-a128-4462924cbc15","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 5. Implementation\n\n### 5.1 Forward Mode Implementation\n\nThe forward mode auto differentiation works for all real number functions.\n\n#### 5.1.1 Core Data Structures\n\n* The user will start by initializing variables using Dual class.\n\n* Numpy Array:\n  Our core data structure of Dual class and its operation is built upon numpy array.\n  The elementary function is also operated on numpy array and real numbers.\n  We store each variable its corresponding value, derivative value in numpy array, such that we could apply elementary operation to entire array instead of just a scalar value.\n  \n* For variables and differentiation point, it will be input in Dual number instantiation as its variable value, and value as the point of differentiation.\n  User would also have to input the length of total variables and the position of current variable in dual class.\n\n* For forward mode, the function is directly built upon operation on dual class initialized above.\n  \n\n#### 5.1.2 Implemented Classes\n\n* `AutoDiff`: The base class for declaring a function that we wish to do auto-differenciation.\n\n* `Forward(AutoDiff)`: Our forward auto-differentiation class. This class implements differention and calculate derivatives and jacobian through forward mode.\n \n* `Dual`: dual number class. This class will take in any real number variable and construct and return it as a dual number,\n  all the subsequent operations in AutoDiff will be done on Dual Number class.\n  \n  The class also contains operations (addition, subtraction, multiplication, division, power, etc) between dual numbers, and real numbers.\n \n\n#### 5.1.3 Implemented Classes\n\n`AutoDiff`:\n\n* Description: This class declares a function to be differentiate. It provides a super method of differentiable function initialization for our Forward, Reverse, Symbolic differentiation classes. \n\n* Attributes: `self.f`: function to be differentiated, it shall be a list, or we initialize a user input as a list. \nWe will pass this function into the child class of `AutoDiff`, which is, for example, `Forward`.\n\n* Methods: \n  \n  *  \\__init__: intilization\n\n`Forward(AutoDiff)`:\n\n* Description: This class initialize a forward AD object, and contains method of getting values, derivatives, and jocobians\n\n* Attributes: `self.f` is the function that we passed in, it is function of variables ready to be differentiated.\n\n* Methods: \n  \n  * \\__init__: intilization, inherit from the super class\n  * get_value: calculate the value of f on var through forward mode on specific value\n  * get_der: calculate the derivative of f on var through forward mode with respect to all variables or specific directions\n  * get_jacobian: calculate the jacobian matrix of f list on all vars through forward mode on all variables, user can specify the direction\n\n`Dual`:\n\n* Description: The Dual class supports custom operations for Automatic Differentiation (AD) in forward mode, it contains overwritten dunder method for basic operations\n(addition, multiplication, exponentiation and so on)\n\n* Attributes:\n\n  `self.val`: The evaluation values of the functions or the initialized values for the variable(dual object)\n\n  `self.der`: The derivatives of the functions or the initialized derivative of the variable. When there are multiple variables to be input in a function, the derivatives will be an\n  array to store the derivatives of different variables separately.\n\n  `self.loc`: The location/index of this variable when there are multiple input variables for the target function(s).\n\n  `self.length`: The length/number of the total variables that will be input when there are multiple input variables for the target function(s).\n\n* Methods:\n \n  * \\__init__: initialize a Dual object, with its values, derivatives, location and length.\n  * \\__repr__: Prints self in the form of Dual(value = [val], derivative = [der])\n  * \\__pos__: Returns the positive of self\n  * \\__neg__: Returns the negative of self\n  * \\__add__: Returns the addition of self and other, other can be Dual object, float, or int\n  * \\__radd__: Returns the addition of other and self, other can be Dual object, float, or int\n  * \\__sub__: Returns the subtraction of self and other, other can be Dual object, float, or int\n  * \\__rsub__:Returns the subtraction of other and self, other can be Dual object, float, or int\n  * \\__mul__: Returns the multiplication of self and other, other can be Dual object, float, or int\n  * \\__rmul__: Returns the multiplication of other and self, other can be Dual object, float, or int\n  * \\__truediv__: Returns the devision of self and other,other can be Dual object, float, or int\n  * \\__rtruediv__: Returns the devision of other and self, other can be float, or int\n  * \\__pow__: Returns the power of self raised by other, other can be Dual object, float, or int\n  * \\__rpow__: Returns the power of other raised by self, other can be float, or int \n  * \\__eq__: Returns boolean if two objects have equal value \n  * \\__ne__: Returns boolean if two objects DO NOT have equal value\n  * \\__lt__: Returns boolean if the former object is less than the latter\n  * \\__le__: Returns boolean if the former object is less than or equal to the latter\n  * \\__gt__: Returns boolean if the former object is greater than the latter\n  * \\__le__: Returns boolean if the former object is greater than or equal to the latter\n\n\n#### 5.1.4 External Dependencies\n\nWe have the following external libraries/Modules to include:\n\n`NumPy`: This provides an API for a large collection of high-level mathematical operations. In addition, it provides support for array operations.\n\n`pytest`: the alternative, more Pythonic way of writing tests, making it easy to write small tests. We plan to use it for a comprehensive test suite\n\nAlso, we will use TravisCI and CodeCov to track the building requirement and code coverage.\n\n\n\n### 5.2 Symbolic Reverse Mode\n\n(For more detail, please consult _6.1 symbolic differentiation_)\n\nExpression class is an abstracted representation of a mathematical function. For instance, some expression represents functions in the form ``a+b``,\nin addition to the arithematic expressions, there are two additional expressions, namly `Symbol` and `Constant`. \n\nEach expression knows how to evaluate itself against a list of given value of each `Symbol`.\n\nEach expression are also implemented to differentiate themselves w.r.t any given `Symbol`, this differentiation returns an expression.\n\nIn this way, as each expression knows how to differentiate itself w.r.t a given `Symbol` and return an expression, which means each expression naturally has the ability of doing higher order function.\n\nTo get value, derivative or jacobian, we just needs to evaluate expression, derivative expression or jacobian expression at their given value.\n\nConceptually, by maintaining a syntax tree, this implementation also meets our definition of `reverse mode differentiation` in the sense that each expression asks its dependencies, combining the results of its dependencies only at the time when `diff()` is called.\n\n\n#### 5.2.1 Core Data Structures\n\n\n* `Expression` class:\n  Our core data structure of will be the Expression class its operations are self-contained.\n\n* Dictionary: the values that the expression will be evaluated against will be entered as a dictionary.\n  \n* NumPy array: We store each variable its corresponding value, derivative value in numpy array, such that we could apply elementary operation to entire array instead of just a scalar value.\n\nFor variables and differentiation point, it will be input in `Symbol` instantiation as its variable value as an dictionary. For symbolic differentiation, the function is directly built upon operation on Expression class initialized above.\n  \n\n#### 5.2.2 Implemented Classes\n\n* `Expression`: The parent class for declaring a Expression object (function) that we wish to do auto-differenciation.\n\n* `Constant(Expression)`: a child class of Expression. The constant is evaluated to be the constant itself and the derivative of constant is zero\nPrinting the class instance will result in a string, which can be concatenated into the Expression.\n\n* `Symbol(Expression)`: a child class of Expression, initiaizing the symbol of variable (x,y,z, etc.), which can be concatenated into the Expression.\n  \n  Note that the parent class also contains operations (addition, subtraction, multiplication, division, power, etc) between dual numbers, and real numbers.\n \n* Operation specific classes: basic arithmatic operation, turns into an sub-class of `Expression`. Including `SumExpression(Expression)`,`ProductExpression`,\n`DivisionExpression`,\n\n  `LnExpression`,`PowerExpression`,\n\n  `SinExpression`,`CosExpression`,`TanExpression`,\n\n  `SinhExpression`,`CoshExpression`,`TanhExpression`,\n\n  `ArcsinExpression`,`ArccosExpression`,`ArctanExpression`,\n\n  In 5.2.3 we will give detail of one Operation specific class to clarify.\n\n\n#### 5.2.3 Implemented Classes\n\n`Expression`:\n\n* Description: The parent class for declaring a Expression object (function) that we wish to do auto-differenciation\n\n* Attributes: `self`, Expression. `values`, a dictionary with key initilizing as variable symbols (x, y, z, etc) and value to be floats that each variable is going to be evaluate against\n\n* Methods: \n  \n  *  evaluate: Evaluate the value of this Expression with the given values of variables\n  *  _symdiff: display the symbolic representation of the derivative of this Expression\n  *  \\__call__: Special method enabling Expression instance to use evalute method and returns the derivative value of the instance\n  *  \\__add__: Addition on Expression\n  *  \\__radd__:Addition (commutative) on Expression\n  *  \\__sub__:Substraction on Expression\n  *  \\__rsub__:Substraction on Expression\n  *  \\__mul__:Multiplication on Expression\n  *  \\__rmul__:Multiplication (commutative) on Expression\n  *  \\__truediv__:Division on Expression\n  *  \\__rtruediv__:Division on Expression\n  *  \\__pow__:Exponentiation on Expression\n  *  \\__rpow__:Exponentiation on Expression\n  *  \\__neg__:Negation on Expression\n  \n  Notice that we DO NOT have to overwrite the comparison methods because the operation in `Expression` class is self-contained, we do not need to compare the value until\n  the very last minute when we want to call the `evaluate` method. If we want to compare at some points, we can call the `evaluate ` method and comapre the values using the \n  built-in default comparison dunder in Python.\n\nAn example of arithmetic Expression class:\n`SumExpression(Expression)`:\n\n* Description: This class declares a function to be differentiate. It provides a super method of differentiable function initialization for our Forward, Reverse, Symbolic differentiation classes. \n\n* Attributes: `self.f`: function to be differentiated, it shall be a list, or we initialize a user input as a list. \nWe will pass this function into the child class of `AutoDiff`, which is, for example, `Forward`.\n\n* Methods: \n  \n  * \\__init__: initialize `self.operand`\n  *  evaluate: Evaluate the value of this Expression with the given values of variables\n  *  _symdiff: display the symbolic representation of the derivative of this Expression\n  * \\__str__: print out the addition expression\n\n#### 5.2.4 External Dependencies\n\nWe have the following external libraries/Modules to include:\n\n`NumPy`: This provides an API for a large collection of high-level mathematical operations. In addition, it provides support for array operations.\n\n`Math`: This provides access to some mathematical functions also.\n\n`__future__.annotations`: for the ease of assigning input/output to certain class\n\n`pytest`: the alternative, more Pythonic way of writing tests, making it easy to write small tests. We plan to use it for a comprehensive test suite\n\nAlso, we will use TravisCI and CodeCov to track the building requirement and code coverage.\n\n\n### 5.3 Handling Elementary Functions\n\nThe elementary model is utilized by both forword implementation and symbolic (reverse) implementation, therefore, we put the sections here. \n  \n For the elementary function, we write our own method of computing the value so that these function can be applied on the Dual number, Expression object, as well as on the real number. \n For example, in our daily usage, sine funtion on a real number `x` can be calculated via `NumPy` (np.sin(x)), but here when we calculate sine value for our self-defined class (Dual, or Expression), \n we cleverly store the value (same as we got from np.sin(x)) and the derivative part. \n\n We have implemented the following elementary functions and we provide a demo function for reference.\n \n\nthe function we have implemented so far, * represents the input\n\n  * Exponentation\n  exp(*): extend the exponential function to an self-defined object\n\n  Note \\__power__(*): in the class dunder method for Dual and PowerExpression class for Expression, we are able to overwrite power in order to exponentiate any base number by any power\n\n  * Square root\n  sqrt(*): #extend the square root function an self-defined object\n\n  * Trig functions\n  sin(*): #extend the sine function to an self-defined object\n\n  cos(*): #extend the cosine function to an self-defined object\n\n  tan(*): #extend the tangent function to an self-defined object\n\n  *  Inverse trig functions\n  arcsin(*): #extend the inverse sine function to an self-defined object\n\n  arccos(*): #extend the inverse cosine function to an self-defined object\n\n  arctan(*): #extend the inverse tangent function to an self-defined object\n\n  * Hyperbolic functions\n  sinh(*)): #extend the hyperbolic sine function to an self-defined object\n\n  cosh(*): #extend the hyperbolic cosine function to an self-defined object\n\n  tanh(*): #extend the hyperbolic tangent function to an self-defined object\n\n  * Logistic function\n  logistic(*): #extend the logistic function to an self-defined object, the default is a standard sigmoid\n\n  * Logarithms\n  log(*): #extend the natural log to an self-defined object\n\n  logb(*,base): #extend the log function with any base to an self-defined object\n\n\n\nHere is one example:\n\n```python\ndef sin(var):\n    \"\"\"Calculate the sine of the input \n\n    Parameters\n    ----------\n    var: Dual, Node, Expression, or real number\n        \n    Returns\n    ------- \n    the sine value: sin{<var>} \n        \n    Examples\n    -------- \n    >>> sin()\n    \"\"\"\n    if isinstance(var, Dual):\n        der = np.cos(var.val) * var.der\n        val = np.sin(var.val)\n        return Dual(val, der)\n\n    elif isinstance(var, Expression):\n        return SinExpression(var)\n\n    else:\n        return np.sin(var)\n```\nIf we call the above function, it will give the following output. \n\n```python\n#...import necessary dependencies\nx = Dual(np.pi, 1)\nz = sin(x)\nprint(z)\n```\nWe will get:\n```python\nDual(value=1.2246467991473532e-16, derivative=-1.0)\n```\n\nNotice z is a Dual object. What if z is an Expression object?\n\n```python\nx = symbols('x')\nz = sin(x ** 2)\nvalues = {x: 3}\nprint(z.evaluate(values))\n```\nWe will get:\n```python\n0.4121184852\n\n```\n\n\nThis function also applies to real number:\n\n```python\n#...import necessary dependencies\nx_real = np.pi\nz_real = sin(x_real)\nprint(z_real)\n\n\n```\nWe will get:\n```python\n1.2246467991473532e-16\n```\n\n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00006-ceec5c5b-eb57-4099-b86e-ba70cb1071eb","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 6. Extensions: Symbolic Reverse Mode \n### 6.1 Future Features from Milestone2 with updates\n\n**Feedback:**\nIt looks like you have thought through some of the implementation details of reverse model. One point of caution: the reverse mode does not have a mathematical correspondance with the dual numbers. The procedure that \nyou describe actually overloads a function value, interprets this overloaded value as a dual number, and then proceeds with creation of the graph required for reverse mode. Again, this is not a dual number implementation of the reverse mode.\" \n\n** Response: **\nThanks! There are mainly three adjustments of our current library and the last future feature sections based on your feedback.\n\n  1. Instead of using Dual class to implement reverse mode, we create a new class Expression to deal with the symbolic of the functions and defferentiation to implement reverse mode.\n\n  2. Also with Expression class, we implemented expression of the function and its derivatives, and also higher order derivatives.\n  \n  3. However, we do not implement user interface and visualization and will add it in the future.\n\n\n* **Jacobian of multiple functions and multiple variables**\n\n  Last time, our package could deal with single function with multiple variables. We now can implement the case when the use will use multiple functions with multiple variables.\n  In this way, we will allow the value and derivatives to be array and combine the derivatives of different functions and variables together.\n\n*  **Reverse Mode of Auto Differentiation class (This is discussed in great details in 6.2)**\n\n  Now our package could calculate deriavtives through forward mode of Auto Differentiation. However, in many tasks in machine learning, it is important to implement reverse mode to customize gradient descent in these problems.\n  To create class of reverse mode. In the final deliveries, we create a new Expression class to store the expression of the variables/sub-functions, and for each operation/elementary function the gradients and its value will be stored.\n  Then we can calculate the derivatives of the function using chain rule to from outer expression to inner one.\n\n*  **Expression and Visualization **\n\n  We actualize the expression and symbolic representation of differentiation, and store them in a tree (or other neccessary data structure), and recombine symbolic representation together after finishing differnetiation.\n  It could requires higher complexity.\n  Besides, if possible, we would also plan to visualize our computational graph in python via visualization package, e.g. d3. \n\n\n\n### 6.2 Background and Information: Reverse Mode\n\nRecall from the background section, we know that reverse mode is another way of autodifferentation and below is the math background.\n\n#### 6.2.1 Explanation and Background\n\nIt should be noticed that in forward mode, chain rule is not utilized. We just follow the evaluation trace and combine the derivatives of elementary functions together. But for reverse mode, we will implement chain rule. \n\nHowever, it is important to realize that the reverse mode also requires the evaluation trace on forward mode to have the derivatives on the elementary functions. Then we will use chain rule to reversely calculate the final derivative.\n\nThe steps we use to implement reverse mode based on the evaluation trace in forward mode is as follows.\n\n- STEP1: Start with $v_1$\n\n   $$\\overline{v_1} = \\dfrac{\\partial f}{\\partial v_1} = 1.$$\n\n- STEP2: Use chain rule to calculate$\\overline{x}$\n\n   $$\\overline{x} = \\dfrac{\\partial f}{\\partial v_1}\\dfrac{\\partial v_1}{\\partial x}  = 1.$$\n\n- STEP3: Get the derivative on x\n\n   $$\\overline{x} = \\dfrac{\\partial f}{\\partial x}  = 1.$$\n\n** Computational graph **\n\nIn reverse mode, we just reversely implement chain rule to get the derivatives. And the computational graph for reverse mode is as follows.\n\n<img src=\"reverse_mode.png\">\n\n*Note: If you cannot see the image, please right click and open image in new tab*\n\n#### 6.2.2 Implementation (Detail in 6.3)\nTo implement reverse mode, we create an Expression class to store the traces in the computational graph, modify elementary functions to customize Expression objects,\n\nConceptually, by maintaining a syntax tree, this implementation also meets the definition of `reverse mode differentiation` in the sense that each expression asks its dependencies, combining the results of its dependencies only at the time differentiation method is called.\n\n\n### 6.3 Symbolic Expression (Reverse Diff and Higher Order Derivative) \nFor extension, we built a Expression class to implement the symbolic representation of our function. \n\n#### 6.3.1 Implementation\nExpression class is an abstracted representation of a mathematical function. For instance, some expression represents functions in the form ``a+b``,\nin addition to the arithematic expressions, there are two additional expressions, namly `Symbol` and `Constant`. \n\nEach expression knows how to evaluate itself against a list of given value of each `Symbol`.\n\nEach expression are also implemented to differentiate themselves w.r.t any given `Symbol`, this differentiation returns an expression.\n\nIn this way, as each expression knows how to differentiate itself w.r.t a given `Symbol` and return an expression, which means each expression naturally has the ability of doing higher order function.\n\nTo get value, derivative or jacobian, we just needs to evaluate expression, derivative expression or jacobian expression at their given value.\n\nConceptually, by maintaining a syntax tree, this implementation also meets our definition of `reverse mode differentiation` in the sense that each expression asks its dependencies, combining the results of its dependencies only at the time when `diff()` is called.\n\n\n#### 6.3.2 Functionality\n* **Evaluate**: An Expression class could be reused and evaluated at given dictionary of values.\n\n* **Differentation**: A Expression class could be differentiated w.r.t a given Symbol, this will return a Expression.\n\n* **Evaluate at Differentiation Expression**: We could evaluate the differentiated expression at a given value, this will return the derivative results w.r.t Symbol specified in Diff().\n\n* **Higher Order Differentation**: This module also naturally supports higher order differentation, in the sense that use could repeatedly call diff() on a expression w.r.t to different variable/symbols, and evaluate its value on a given value dictionary. \n\n* **Get Jacobian Matrix**: This module could also outputs a Jacobian Matrix on a list of expressions, for a list of Symbols.\n\n#### 6.3.3 Usage\n\nFirst Step: User instantiate variables.\nYou can choose to initialize by wrapper method for multiple variables together.\nOr you could initialize indivial symbols by Symbol class.\n\n```Python\nx, y, z = symbols('x y z')\nx1 = Symbol('x1')\n```\n\nSecond Step: User inputs function, based on above variables\n```Python\nf2 = (tanh(cos(sin(y))**z) + logistic(z**z, 2, 3, 4))**(1/x)\n```\n\nThird Step: User input the values of the variables\n```Python\nvalues = {x: 2, y: np.pi, z: 4}\n```\n\nFourth Step: User could choose to call instance method evaluate() to get value of func\n```Python\nprint(f2.evaluate(values))\n```\n\nFifth Step: User could choose to call instance method diff() to get first order derivative or higher order derivative of func\n*get derivative of f1 with respect to z*\n```Python\nprint(diff(f2, z).evaluate(values))\n```\n\n*get second order derivative of f2 with respect to z*\n```Python\nprint(diff(f2, z, z).evaluate(values))\n```\n\n*get partial derivative of f2: df2/dxdy*\n```Python\nprint(diff(f2, x, y).evaluate(values))\n```\n\n*get third derivative of f with respect to x*\n```Python\nprint(diff(f2, x, x, x).evaluate(values))\n```\n\nSixth Step: User could User could get jacobian/derivatives of multiple functions with multiple variables\n```Python\nf1 = 3 * x + 4 * y * 2 - z\nf3 = exp(arccos(tan(sin(y))) + logb(z**(1/2), 1/5)*sinh(x))\n```\n\nUser could get Jacobian Matrix with method get_jacobian_value()\n\n*Note: the order displayed in the Jacobian Matrix is matched with the order of input functions(as row) and the input variables(as column)*\n```Python\nprint(get_jacobian_value([f1, f2, f3], [x, y, z], values))\n```\n\nSeventh Step: User could get the expression of the function\n```Python\nprint(f1)\n```\n\nUser could also get the expression of (higher order) derivatives\n```Python\nprint(diff(f2, x))\nprint(diff(f2, x, y))\n```\n","metadata":{"tags":[],"cell_id":"00007-efc75aa3-c4f6-4aad-8c72-d075b7eb9db3","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 7. Broader Impact and Inclusivity Statement \n\n### Broader Impact\n\nAs a open source library, it is important to care about the broader impact on the social community, especially in terms of diversity, ethical and social impact. \n\nAbove all, we, the developers for AutoDiff, spare no effort to encourage the autonomy and freedom of using and contributing to the library from diversified groups, especially for the women, people with disability and working parents. We hope our library can provide great motivation and encouragement for the minority groups in contributors and for other open source library, contributing to the diversity in the whole open source community.\n\nAdditionally, the open source Auto Differentiation library may also be misused in some scenarios and cause some ethical issues. For one thing, there may be some students or researchers to overuse this library instead of calculating the derivatives by hand. Although AutoDiff can be efficient and powerful for solving complicated gradient problems, sometimes it is also vital for students to learn how to conduct the derivatives by hand, and for mathematical/physical researchers to discover drawbacks and make breakthroughs. Therefore, it should be notified that AutoDiff is only a tool, but not a bible to entirely depend on. For the other, there may be some people using AutoDiff for business purpose or offering it for sale, which is entirely against our purpose and privacy rules. It should be warned that our library is designed for academical purpose, increasing the efficiency of Machine Learning tasks and other gradient-related problems.\n\n\n\n### Software Inclusivity\nSoftware development, like many fields of science, has been prosperous because the contribution of people from a variety of backgrounds. This package welcomes and encourage participation and usage from a global community. \nJust as Python Software's Diversity Statement indicated, *the Python community is based on mutual respect, tolerance, and encouragement, and we are working to help each other live up to these principles.* We, as the developer of this AutoDiff package, also want our user group to be more diverse: whoever you are, and whatever your background, we welcome you to use our package.\nThis software package is built based upon the diversity perspective on Python broader community. We strongly believe that embrace diverse community to use our package brings new blood and perspective, making our user group stronger and more vibrant. A diverse user group where all users treat each other with respect has more potential contributors and more sources for fresh ideas.\nWe also welcomes users from all language background. Mathematics has no boundary.\n\nIn principle, there should be no barrier whatsoever for other developers to contribute to our code base. \nIn practice, these barriers do exist and could be rather subtle. \n\nOur software project will be mainly published on Github page, and welcomes contributions from opening issues and pull requests from a broad audiences. \nWe will also leave our email (email address could be found at **README.md**) for reaching out for closer contact if any users has ideas on contributing but found it difficult to overcome barriers. With a closer communication, we could reach out to our users and accomodate for their needs accordingly if that would help.\n\nPull requests will be reviewed and approved together by each of our group members. Emails will also be reviewed and approved together.\n\nFor underrepresented group, we welcome contributions from their perspective and are willing to receive their comments and feedbacks. We will also remove sensitive content and words in our code base, for example, we would be taking care of code and variable naming to avoid use of words like `blacklist` or `whitelist`, `master` or `slave`, but to use a more generic term.\nWe will also rename our main branch to `main`.\n\nFor working parents, we will accomodate for their time if they would like a closer contact and discuss about our package implementation. \nFor people from different countries or non-native English speakers, rural communities, we would be willing to receive their feedback and contribution through all channels. We could be reached via email, phone, letter or any other means plausible.\nWe will also use translator to help accomodate for language barriers if it is needed.\n\nFor people with disabilities, we will be happy to provide accommodations, inluding but not limiting to sign language explanation and hearing assitance. We are happy to go with more detail if you feel needed.\n\nWe want to make our biggest effort to create an inclusive learning and communication environment for all the users and members from the science community that supports a diversity of thoughts, perspectives, and experiences, and honors the identities.\nTo help achieve it:\n\n* If you feel anything such as the name and the written records that disturb you in this software, please let us know.\n\n* If you feel anything improper in our future maintanance and development process, please let us know\n\n* As we welcome everyone to contribute, we shall all strive to honor the diversity of each other\n\n","metadata":{"tags":[],"cell_id":"00008-fd61fef8-267c-4a0c-bc92-9d9c90e0f5b4","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# 8. Futures\n\nFor future improvments, below are some points we have in mind:\n* **Expression class simplification**: We maintained syntax tree for expression class, however, it is not a very polished version for read-friendly expression. Especially when there's a complex function, or when we differentiated for several times and get a higher order differentiation expression. \n    * Simplify Adding: When there's multiple expression adding together, especially constant adding expression adding constant, we should simply and adding constants together.\n    * Simplify Multiplication: Similar to adding, remove multi-layer mulitiplication.\n    * Constant: Constant arithmetic\n    * Symbols: Multiple same Symbol operations could be combined, e.g `x+x+x+x` could be simplified as `4*x`\n    * Zero * Symbol: should return Constant Zero itself\n    * One * Symbol: should return Symbol itself\n    * Symbol ** 1: should return Symbol itself\n    * A lot more other arithmetic simplification for better readibility on Expression...\n\n\n* **Implement User Interface with elegant input (or Web Development)**\n\n  Now that our package's Forward mode requires the user to initialize the variables using Dual class with specified location and length of the input variables. \n  And then uer could use these Dual objects to create the target functions. However, there are mainly two future improvements that can be implemented to make the package more friendly to users.\n\n  1. With user entering the function expression, our package will automatically parse the function break it down into elementary functions and store them into tree structure.\n\n    When users want to repeatively use the functions that require our package to calculate the deriavtives, now the users have to redefine the funtions repeatively.\n    Once we could parse the function expressions and use tree structure to store the operations and input variables at each step, we can free users from initializing functions and variables repeatively.\n\n  2. With user directly entering the input variables for the target functions, our package will automatically initialize the Dual objects that will be used in the target functions.\n  \n    When users initialize the variables, they should first think about how many variables will be used in the target functions. But sometimes users may want to use the variables later in different functions.\n    Therefore, in the future we will use list to store the values that the user is intended to use in the function instead of the initialized variables. Our package will automatically initialize the variables for the user\n    with these values.\n  3. We could also aim to develop a web based application for user to input in GUI as an easier use case.","metadata":{"tags":[],"cell_id":"00009-02e1abdc-6bff-43f5-9d3a-7ffa72357e96","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 9. Reference\n\n[[1]](https://www.jmlr.org/papers/volume18/17-468/17-468.pdf): Baydin, Atilim Gunes; Pearlmutter, Barak; Radul, Alexey Andreyevich; Siskind, Jeffrey (2018). \"Automatic differentiation in machine learning: a survey\". Journal of Machine Learning Research. 18: 1–43.\n\n[[2]](https://fac.ksu.edu.sa/sites/default/files/numerical_analysis_9th.pdf):Rirchard L. Burden and J. Douglas Faires. Numerical Analysis. Brooks/Cole, 2001.\n\n[[3]](https://www.springer.com/gp/book/9783540654667):Johannes Grabmeier and Erich Kaltofen. Computer Algebra Handbook: Foundations, Applications, Systems. Springer, 2003\n\n[[4]](https://www.jstor.org/stable/24103956): Arun Verma. An introduction to automatic differentiation. Current Science, 78(7):804–7,\n2000.\n\n[[5]](https://www.worldcat.org/oclc/31441929):  Spivak, Michael. (1994). *Calculus* (3rd ed.). Houston, Tex.: Publish or Perish. p. 359.\n\n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00007-0bbd384f-b3d5-48d3-8505-35b35eaea174","deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"64662b78-00b4-4022-a0e3-4105838ce0f1","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}}}