{"cells":[{"cell_type":"code","metadata":{"output_cleared":false,"source_hash":"9e5190f0","execution_millis":120,"cell_id":"00000-b743eac2-5bbf-4003-b687-9aa1017b5594","execution_start":1607504841760,"deepnote_cell_type":"code"},"source":"# RUN THIS CELL FOR FORMAT\nimport requests\nfrom IPython.core.display import HTML\nstyles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\nHTML(styles)","execution_count":1,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\nblockquote { background: #AEDE94; }\nh1 { \n    padding-top: 25px;\n    padding-bottom: 25px;\n    text-align: left; \n    padding-left: 10px;\n    background-color: #DDDDDD; \n    color: black;\n}\nh2 { \n    padding-top: 10px;\n    padding-bottom: 10px;\n    text-align: left; \n    padding-left: 5px;\n    background-color: #EEEEEE; \n    color: black;\n}\n\ndiv.exercise {\n\tbackground-color: #ffcccc;\n\tborder-color: #E9967A; \t\n\tborder-left: 5px solid #800080; \n\tpadding: 0.5em;\n}\n\ndiv.exercise-r {\n\tbackground-color: #fce8e8;\n\tborder-color: #E9967A; \t\n\tborder-left: 5px solid #800080; \n\tpadding: 0.5em;\n}\n\n\nspan.sub-q {\n\tfont-weight: bold;\n}\ndiv.theme {\n\tbackground-color: #DDDDDD;\n\tborder-color: #E9967A; \t\n\tborder-left: 5px solid #800080; \n\tpadding: 0.5em;\n\tfont-size: 18pt;\n}\ndiv.gc { \n\tbackground-color: #AEDE94;\n\tborder-color: #E9967A; \t \n\tborder-left: 5px solid #800080; \n\tpadding: 0.5em;\n\tfont-size: 12pt;\n}\np.q1 { \n    padding-top: 5px;\n    padding-bottom: 5px;\n    text-align: left; \n    padding-left: 5px;\n    background-color: #EEEEEE; \n    color: black;\n}\nheader {\n   padding-top: 35px;\n    padding-bottom: 35px;\n    text-align: left; \n    padding-left: 10px;\n    background-color: #DDDDDD; \n    color: black;\n}\n</style>\n\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Documentation\n@merlionctc\n\n**Table of Contents**\n- [1. Introduction](#1.-Introduction)\n  - [1.1 Derivative](#1.1-Derivatives)\n  - [1.2 Automatic Differentiation](#1.2-Auto-Differentiation)\n- [2. Background](#2.-Background)\n  - [2.1 Chain Rule](#2.1-Chain-Rule) \n  - [2.2 Elementary functions](#2.2-Elementary-functions) \n  - [2.3 Forward mode](#2.3-Forward-mode) \n  - [2.4 Reversed mode](#2.4-Reversed-mode) \n  - [2.5 Dual Number](#2.5-Dual-Number) \n- [3. Usage](#3.-Usage)\n  - [3.1 Installation](#3.1-Installation)\n  - [3.2 How to use](#3.2-How-to-Use)\n  - [3.3 Notes](#3.3-Notes)\n- [4. Software organization](#4.-Software-Organization)\n- [5. Implementation](#5.-Implementation)\n- [6. Extensions](#6.-Extensions)\n- [7. Broader Impact and Inclusivity Statement](#7.-Broader-Impact-and-Inclusivity-Statement)\n- [8. Reference](#8.-Reference)\n\n\n\n\n\n## 1. Introduction\n\nWe developed this package, `AutoDiff`,  in the light of automatic differentiation. The package can help to automatically differentiate a function input into the program. The package includes modules of forward-mode differentiation and backward-mode differentiation.\n\n### 1.1 Derivatives\n\nFormally, for single variable case, the derivative of a function, if it exists, is defined as\n\n$$ f'(x) = \\lim_{h\\to0} \\frac{f(a+h) - f(a)}{h} $$\n\nto visualize, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value.\n\nOf course, derivatives may be generalized to functions of several real variables, and derivatives are useful in finding the maxima and minima of functions. Derivatives has a variety of applications in statistics and machine learning, and the process of finding a derivative is called *differentiation*.\n\nThere are three ways of differentiation realized in computer science:\n- Numerical differentiation\n- Symbolic differentiation\n- Automatic differentiaton\n\n\n### 1.2 Auto Differentiation\nAutomatic differentiation (AD), also called as algorithmic differentiation, is a set of techniques for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs <sup>1</sup>. It is not numerical differentiation, since numerical differentiation is the finite difference approximation of derivatives using the values of the original function evaluated at some sample points<sup>2</sup>. It is different from symbolic differentiation, since symbolic differentiation is the automatic manipulation of expressions for obtaining derivative expression<sup>3</sup>.\n\nThe essence of AD is that all numerical computations are ultimately compositions of a finite set of elementary operations for which the derivatives are easily known<sup>4</sup>. The algorithm of AD breaks down a function by looking at the sequence of elementary arithmetic operations (addition, subtraction, multiplication and division) and elementary functions (exponential, logrithmatic, and trigonometry). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to machine accuracy.\n\nThis differentiation technique well-established and used with applications in different areas such as fluid dynamics, astronomy, and engineering design optimization.\n\nTo sum up, there are two major advantages of using AD:\n- Computes derivatives to machine precision.\n- Does not rely on extensive mathematical derivations or expression trees, so it is easily applicable to a wide class of functions.\n\n## 2. Background\n\nAutomatic differentiation relies on several vital mathematical foundations, some of which will be illustrated at this part. Based on these conceptions, it will be more resonable for users to understand the software.\n\n### 2.1 Chain Rule\n\nChain Rule is the most important concept in AD. It enables us to deal with complex functions with several layers and arguments. With implementing chain rule, we can easily divide the original complicated functions into basic parts made up with elementary functions, of which we will know the concrete derivative expressions.\n\nSuppose there is a function $h\\left(u\\left(t\\right)\\right)$ and in order to calculate derivative of $h$ with respect to $t$, we should use chain rule. The derivative is $$\\dfrac{\\partial h}{\\partial t} = \\dfrac{\\partial h}{\\partial u}\\dfrac{\\partial u}{\\partial t}.$$\n\nIn general, if a function $h$ has several arguments, or even its argument is a vector, so that $h = h(x(t))$ where  $x \\in R^n$ and $t \\in R^m $. In this way, $h$ is now the combination of $n$ functions, each of which has $m$ variables. The derivative of $h$ is now\n\n $$\\nabla_{t}h = \\sum_{i=1}^{n}{\\frac{\\partial h}{\\partial x_{i}}\\nabla y_{i}\\left(t\\right)}.$$\n\n### 2.2 Elementary functions\n\nAny complex function is made up with several elementary functions. As discussed above, we use chain rule to break it down and then focus on elementary functions to calulate their derivatives. \n\nIn mathematics, an elementary function is a function of a single variable composed of particular simple functions. Elementary functions are typically defined as a sum, product and/or composition of many polynomials, rational functions, trigonometric and exponential functions, and their inverse functions.<sup>5</sup>\n\nOn the other hand, we know the concrete mathematical expression of the elementary functions, which will be used directly in the later graph structure of calculations. \n\n\n### 2.3 Forward mode\n\n#### 2.3.1 Evaluation trace\n\nTake the example of $g = (x+y)*z$, we will first demonstrate the evaluation trace and then its corresponding computational graph.\n\nLet's evaluate g at the point (1,1,1). In the evaluation trace table, we will record the trace of each step, its  elementary operation as well as the corresponding numeric value at the point.\n\n| Trace | Elementary Operation | Numeric Value |\n| ----- | -------------------- | ------------- |\n| $x$   | 1                    | 1             |\n| $y$   | 1                    | 1             |\n| $z$   | 1                    | 1             |\n| $p$   | $x+y$                | 2             |\n| $f$   | $v_1*z$              | 2             |\n\n#### 2.3.2 Computational graph\n\nThe above evaluation trace can be easily visualized with the computational graph below. The node will represent the trace and the edge will represent the elementary operation.\n\n<img src=\"forward_mode.png\">\n\n*Note: If you cannot see the image, please right click and open image in new tab*\n\n#### 2.3.3 Explanation\nThe evaluation trace above is just the path we will follow in forward mode. On top of that, we will also carry the derivatives. And we will take the derivative of g on x.\n\n| Trace | Elementary Operation | Numeric Value | Deri. on x    | Deri. Value on x |\n| ----- | -------------------- | ------------- | ------------- | ---------------- |\n| $x$   | 1                    | 1             | 1             | 1                |\n| $y$   | 1                    | 1             | 0             | 0                |\n| $z$   | 1                    | 1             | 0             | 0                |\n| $v_1$ | $x+y$                | 2             | $\\dot{x}$     | 1                |\n| $f$   | $v_1*z$              | 2             | $\\dot{v_1}*z$ | 1                |\n\n ### 2.4 Dual Number\n \n A dual number has a real part and a dual part. Say we have $z = a + b\\epsilon $. Then $a$ is the real part and $b$ is the dual part.\n\n For $\\epsilon$, we define $\\epsilon ^2 = 0$ but $\\epsilon$ is not zero.\n \n Dual Number is really useful when we want to calculate derivatives of a function. For example, say we have\n\n $$ x = a + b\\epsilon,   y = x^2$$\n \n Then we can derive\n \n $$y = (a + b\\epsilon)^2 = a^2 + 2ab\\epsilon + b^2\\epsilon^2 = a^2 + 2ab\\epsilon$$\n\n Therefore, it is really convenient to get the value of y from real part and get derivative of y from dual part. This is what we will implement in our FORWARD auto differentiation code.\n \n \n\n\n","metadata":{"output_cleared":false,"tags":[],"cell_id":"00001-38382959-638d-48ce-a914-b41306f3057c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 3. Usage\n\n\n ### 3.1 Installation\n\nThere are two ways to install the `AutoDiff` package. For milestone 2, please see method 2. \nIn the end, the package will be distributed through PyPI (Not yet implemented, a usage example is shown on demo.py).\n\n* Method 1: install packaged using pip (This is a future implementation, because this time we have not yet uploaded our package to cloud)\n\n    To install AutoDiff using pip, in the terminal, type:\n\n    ```bash\n    pip install AutoDiff\n    ```\n\n    This will also install all required documents as dependency.\n\n    Run module tests before beginning,\n    \n    First, navigate to our package distribution website, download tar.gz folder, unzip, and enter the folder\n    \n    ```bash\n    pytest tests\n    ```\n    Use AutoDiff package \n    \n    ```python\n    >>> import AutoDiff.autodiff as ad\n    >>> from ad import *\n    >>> import numpy as np\n    #（begin autodifferentiation)\n    >>> quit()\n    ```\n\n\n* Method 2: install using virtual environment\n    \n    For now, to get started, please do git clone on our project and test by using `demo.py`\n\n    First, download the package from github to your folder\n\n    ```bash\n    mkdir test_merlionctc\n    cd test_merlionctc\n    git clone https://github.com/merlionctc/cs107-FinalProject.git\n    cd cs107-FinalProject\n    ```\n\n    Create a vertual environment and activate it\n\n    ```bash\n    # If you do not have virtualenv, install it\n    sudo easy_install virtualenv\n    # Create virtual environment\n    virtualenv ac207\n    # Activate your virtual environmeny\n    source ac207/bin/activate\n    ```\n\n    To ensure you have your enviroment and all required package setup, \n\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n    To install our package while using \n    \n    ```bash\n    pip install ./AutoDiff/dist/autodiff_merlionctc-0.0.1-py2.py3-none-any.whl\n    ```\n\n    To run a demo we provided.\n\n    ```bash\n    python3 AutoDiff/demo.py\n    ```\n    \n    If you want to quit the virtual enviornment:\n\n    ```bash\n    deactivate\n    ```\n    \n\n### 3.2 How to Use\n\nHere is an example that serves that a quick start tutorial.\n\nAfter installing AutoDiff package (see section 3.1 Method 2)\n\n```python\npython\n>>> from autodiff.dual import *\n>>> from autodiff.elementary import *\n>>> from autodiff.model import *\n>>> import numpy as np\n```\n\nFirst Step: User instantiate variables\n* val: value of variable that you start with\n* der: value of the derivative of variable that you start with, usually starting with 1\n* loc: The location/index this variable when there are multiple input variables for the target function(s). For example, if you initialize x1 first, the loc will be 0; then you initialize y1, the loc will increment to 1\n* length: The length/number of the total variables that will be input when there are multiple input variables for the target function(s).For example, if you want to initialize x1,y1 and z1, the length will be 3, for each variable in the initialization process\n\n```python\n>>>x1 = Dual(val = 1, der=1, loc = 0, length = 3)\n>>>y1 = Dual(val = 2, der=1, loc = 1, length = 3)\n>>>z1 = Dual(val = 5, der=1, loc = 2, length = 3)\n```\nSecond Step: User inputs function, based on above variables\n```python\n>>>f1 = 3 * x1 + 4 * y1 * 2 - cos(z1)\n```\n\nThird Step: User instantiate `autodiff.Forward` class \n```python\n>>>fwd_test = Forward(f1)\n```\n\nFourth Step: User could choose to call instance method `get_value()` to get value of func\n```python\n>>>print(fwd_test.get_value())\n18.716337814536775\n```\n\nFifth Step: User could choose to call instance method `get_der()` to get derivatives of func\n\nNote: This method will return a derivative vector w.r.t to ALL variables. \n\nNote 2: If user enters a scalar function, then get_der will return the jacobian\n\n```python\n>>>print(fwd_test.get_der())\n[ 3.          8.         -0.95892427]\n```\n\nSixth Step: User could choose to call instance method get_der(var) to get derivatives of func\n\nNote: This method will return a derivative vector w.r.t to specific variables you input\n\n```python\n>>>print(fwd_test.get_der(x1))\n[3.0]\n```\n\nquitting\n```python\n>>>quit()\n```\n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00004-dae35070-7695-4cdd-9719-7ceb3858b634","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 4. Software Organization\n\nDiscuss how you plan on organizing your software package. We provided a basic structure here. \n\n* Directory Structure\n\n```\nAutoDiff\n│   README.md\n│   .travis.yml  \n│   .coverage\n│   requirements.txt\n│\n└───src\n│   │\n│   └─── autodiff\n│   │   │   \n│   │   └─── symbolic\n│   │   │   │  expression.py (symbolic differentiation)\n│   │   │\n│   │   │   model.py (AutoDiff main class with Forward/Reverse mode)\n│   │   │   elementary.py (Elementary Function module)\n│   │   │   dual.py (Dual Number Class)\n│   │   │   node.py (Node Class)\n│        ...\n│\n└───tests\n│   │   test_autodiff.py\n│   │   test_dual_class.py\n│   │   test_elementary.py\n│   │   test_node_class.py\n│   │   test_symbolic.py\n│   │   ...\n└───Docs\n│   │   README.md\n│   │   milestone1.ipynb\n│   │   milestone2_progress.ipynb\n│   │   milestone2.ipynb\n│   │   documentation.ipynb\n│   │   ...\n│      \n│   \n└───demo.py (A demo to usage of package)\n\n```\n\n* **Modules to Include**\n\n Standrad Python libraries/modules:\n\n *math*: mathematical, algebric operations\n\n *numpy*: supports computations for large, multi-dimensional arrays and matrices. \n\n Self-designed libraries/modules under `AutoDiff` package:\n\n *elementary*: contains overwritten elementary functions customized for the different variable class defined when implementing different types of differentiation.\n\n *model*: An interface-like, main class with Forward/Reverse mode auto-differentation methods. Also, the methods are useable for symbolic differentiation \n\n *dual*: This module contains class methods for dual number class, which is the basic class structure in the forward mode\n\n *node*: This module contains class methods for node class, which is the basic class structure in the reverse mode\n\n *expression*: This module contains class methods and child classes of expression class, which is the basic class structure of symbolic differentiation\n\n\n* **Test Suite Design**\n\n We used Pytest as our test suite. We tested all of our implemented classes on every single instance methods.\n We also tested on special and corner cases such as simplication, real number etc. \n The whole test suite is included in the *test* sub-directory. \n And both TravisCI and Coveralls will be used to check the codes coverage and test integration.\n\n* **Package Distribution **\n\n  * Package Distribution\n\n    We will distribute our software using [Python Package Index (PyPi)](https://pypi.org/).\n    Currently, we used Python project structure from [PyScaffold](https://pypi.org/project/PyScaffold/).\n\n  * Software package\n\n    We will package using the standard packaging tool ([setuptools](https://packaging.python.org/key_projects/#setuptools)), and following Package Python Projects [Tutorial](https://packaging.python.org/tutorials/packaging-projects/).\n\n  As mentioned in 3.1 **_How to Install_**, the user can either choose to use `pip install` or installing using `github clone` to install the necessary dependencies.\n\n  \n\n\n\n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00005-85d314d9-e4fb-4bc4-a128-4462924cbc15","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 5. Implementation\n\n### 5.1 Forward Mode Implementation\n\nThe forward mode auto differentiation works for all real number functions.\n\n#### 5.1.1 Core Data Structures\n\n* The user will start by initializing variables using Dual class.\n\n* Numpy Array:\n  Our core data structure of Dual class and its operation is built upon numpy array.\n  The elementary function is also operated on numpy array and real numbers.\n  We store each variable its corresponding value, derivative value in numpy array, such that we could apply elementary operation to entire array instead of just a scalar value.\n  \n* For variables and differentiation point, it will be input in Dual number instantiation as its variable value, and value as the point of differentiation.\n  User would also have to input the length of total variables and the position of current variable in dual class.\n\n* For forward mode, the function is directly built upon operation on dual class initialized above.\n  \n\n#### 5.1.2 Implemented Classes\n\n* `AutoDiff`: The base class for declaring a function that we wish to do auto-differenciation.\n\n* `Forward(AutoDiff)`: Our forward auto-differentiation class. This class implements differention and calculate derivatives and jacobian through forward mode.\n \n* `Dual`: dual number class. This class will take in any real number variable and construct and return it as a dual number,\n  all the subsequent operations in AutoDiff will be done on Dual Number class.\n  \n  The class also contains operations (addition, subtraction, multiplication, division, power, etc) between dual numbers, and real numbers.\n \n\n#### 5.1.3 Implemented Classes\n\n`AutoDiff`:\n\n* Description: This class declares a function to be differentiate. It provides a super method of differentiable function initialization for our Forward, Reverse, Symbolic differentiation classes. \n\n* Attributes: `self.f`: function to be differentiated, it shall be a list, or we initialize a user input as a list. \nWe will pass this function into the child class of `AutoDiff`, which is, for example, `Forward`.\n\n* Methods: \n  \n  *  \\__init__: intilization\n\n`Forward(AutoDiff)`:\n\n* Description: This class initialize a forward AD object, and contains method of getting values, derivatives, and jocobians\n\n* Attributes: `self.f` is the function that we passed in, it is function of variables ready to be differentiated.\n\n* Methods: \n  \n  * \\__init__: intilization, inherit from the super class\n  * get_value: calculate the value of f on var through forward mode on specific value\n  * get_der: calculate the derivative of f on var through forward mode with respect to all variables or specific directions\n  * get_jacobian: calculate the jacobian matrix of f list on all vars through forward mode on all variables, user can specify the direction\n\n`Dual`:\n\n* Description: The Dual class supports custom operations for Automatic Differentiation (AD) in forward mode, it contains overwritten dunder method for basic operations\n(addition, multiplication, exponentiation and so on)\n\n* Attributes:\n\n  `self.val`: The evaluation values of the functions or the initialized values for the variable(dual object)\n\n  `self.der`: The derivatives of the functions or the initialized derivative of the variable. When there are multiple variables to be input in a function, the derivatives will be an\n  array to store the derivatives of different variables separately.\n\n  `self.loc`: The location/index of this variable when there are multiple input variables for the target function(s).\n\n  `self.length`: The length/number of the total variables that will be input when there are multiple input variables for the target function(s).\n\n* Methods:\n \n  * \\__init__: initialize a Dual object, with its values, derivatives, location and length.\n  * \\__repr__: Prints self in the form of Dual(value = [val], derivative = [der])\n  * \\__pos__: Returns the positive of self\n  * \\__neg__: Returns the negative of self\n  * \\__add__: Returns the addition of self and other, other can be Dual object, float, or int\n  * \\__radd__: Returns the addition of other and self, other can be Dual object, float, or int\n  * \\__sub__: Returns the subtraction of self and other, other can be Dual object, float, or int\n  * \\__rsub__:Returns the subtraction of other and self, other can be Dual object, float, or int\n  * \\__mul__: Returns the multiplication of self and other, other can be Dual object, float, or int\n  * \\__rmul__: Returns the multiplication of other and self, other can be Dual object, float, or int\n  * \\__truediv__: Returns the devision of self and other,other can be Dual object, float, or int\n  * \\__rtruediv__: Returns the devision of other and self, other can be float, or int\n  * \\__pow__: Returns the power of self raised by other, other can be Dual object, float, or int\n  * \\__rpow__: Returns the power of other raised by self, other can be float, or int \n  * \\__eq__: Returns boolean if two objects have equal value \n  * \\__ne__: Returns boolean if two objects DO NOT have equal value\n  * \\__lt__: Returns boolean if the former object is less than the latter\n  * \\__le__: Returns boolean if the former object is less than or equal to the latter\n  * \\__gt__: Returns boolean if the former object is greater than the latter\n  * \\__le__: Returns boolean if the former object is greater than or equal to the latter\n\n### 5.2 Symbolic Reverse Mode\n\n(For more detail, please onsult _6.1 symbolic differentiation_)\n\nExpression class is an abstracted representation of a mathematical function. For instance, some expression represents functions in the form ``a+b``,\nin addition to the arithematic expressions, there are two additional expressions, namly `Symbol` and `Constant`. \n\nEach expression knows how to evaluate itself against a list of given value of each `Symbol`.\n\nEach expression are also implemented to differentiate themselves w.r.t any given `Symbol`, this differentiation returns an expression.\n\nIn this way, as each expression knows how to differentiate itself w.r.t a given `Symbol` and return an expression, which means each expression naturally has the ability of doing higher order function.\n\nTo get value, derivative or jacobian, we just needs to evaluate expression, derivative expression or jacobian expression at their given value.\n\nConceptually, by maintaining a syntax tree, this implementation also meets our definition of `reverse mode differentiation` in the sense that each expression asks its dependencies, combining the results of its dependencies only at the time when `diff()` is called.\n\n\n#### 5.2.1 Core Data Structures\n\n\n* `Expression` class:\n  Our core data structure of will be the Expression class its operations are self-contained.\n\n* Dictionary: the values that the expression will be evaluated against will be entered as a dictionary.\n  \n* NumPy array: We store each variable its corresponding value, derivative value in numpy array, such that we could apply elementary operation to entire array instead of just a scalar value.\n\nFor variables and differentiation point, it will be input in `Symbol` instantiation as its variable value as an dictionary. For symbolic differentiation, the function is directly built upon operation on Expression class initialized above.\n  \n\n#### 5.2.2 Implemented Classes\n\n* `Expression`: The parent class for declaring a Expression object (function) that we wish to do auto-differenciation.\n\n* `Constant(Expression)`: Our forward auto-differentiation class. This class implements differention and calculate derivatives and jacobian through forward mode.\n \n* `Symbol(Expression)`: dual number class. This class will take in any real number variable and construct and return it as a dual number,\n  all the subsequent operations in AutoDiff will be done on Dual Number class.\n  \n  The class also contains operations (addition, subtraction, multiplication, division, power, etc) between dual numbers, and real numbers.\n \n* Operation specific classes: basic arithmatic operation, turns into an sub-class of `Expression`. Including `SumExpression(Expression)`,`ProductExpression`,\n`DivisionExpression(Expression)`\n\n#### 5.1.3 Implemented Classes\n\n`AutoDiff`:\n\n* Description: This class declares a function to be differentiate. It provides a super method of differentiable function initialization for our Forward, Reverse, Symbolic differentiation classes. \n\n* Attributes: `self.f`: function to be differentiated, it shall be a list, or we initialize a user input as a list. \nWe will pass this function into the child class of `AutoDiff`, which is, for example, `Forward`.\n\n* Methods: \n  \n  *  \\__init__: intilization\n\n\n\n\n  * **Elementary Function module** \n  \n For the elementary function, we write our own method of computing the value so that these function can be applied on the Dual number, as well as on the real number. \n For example, in our daily usage, sine funtion on a real number `x` can be calculated via `NumPy` (np.sin(x)), but here when we calculate sine value for a Dual object, \n we cleverly store the value (same as we got from np.sin(x)) and the derivative part. \n\n We have implemented the following elementary functions and we provide a demo function for reference.\n \n```python\n#the function we have implemented so far, * represents the input\n\n# Exponentials\nexp(*): #extend the exponential function to Dual number\n\n# Square root\nsqrt(*): #extend the square root function with base e to Dual number\n\n# Trig functions\nsin(*): #extend the sine function with base e to Dual number\ncos(*): #extend the cosine function with base e to Dual number\ntan(*): #extend the tangent function with base e to Dual number\n\n# Inverse trig functions\narcsin(*): #extend the inverse sine function with base e to Dual number\narccos(*): #extend the inverse cosine function with base e to Dual number\narctan(*): #extend the inverse tangent function with base e to Dual number\n\n# Hyperbolic functions\nsinh(*)): #extend the hyperbolic sine function with base e to Dual number\ncosh(*): #extend the hyperbolic cosine function with base e to Dual number\ntanh(*): #extend the hyperbolic tangent function with base e to Dual number\n\n# Logistic function\nlogistic(*): #extend the logistic function with base e to Dual number, the default is a standard sigmoid\n\n#Logarithms\nlog(*): #extend the natural log to Dual number\nlogb(*,base): #extend the log function with any base to Dual number\n\n```\n\nHere is the demo:\n\n```python\ndef sin(dual):\n    \"\"\"Calculate the sine of the input\n        Keyword arguments:\n        dual -- a dual number or a real number\n        Return:\n        the sine value\n    \"\"\"\n    if isinstance(dual, Dual):\n        der = np.cos(dual.val)*dual.der\n        val = np.sin(dual.val)\n        return Dual(val,der)\n    else:\n        return np.sin(dual)\n             \n```\nIf we call the above function, it will give the following output. \n\n```python\n#...import necessary dependencies\nx = Dual(np.pi, 1)\nz = sin(x)\nprint(z)\n```\nWe will get:\n```python\nDual(value=1.2246467991473532e-16, derivative=-1.0)\n\n```\n\nNotice z is a Dual object. This function also applies to real number:\n\n```python\n#...import necessary dependencies\nx_real = np.pi\nz_real = sin(x_real)\nprint(z_real)\n\n\n```\nWe will get:\n```python\n1.2246467991473532e-16\n```\n\nIn the future, we may consider extending our elementary function libraries, including the hypobolic sine and cosine, as well as functions like csc and cot, log with base 2 and 10, if needed.\n\n* **External Dependencies**\n\nWe have the following external libraries/Modules to include:\n\n`NumPy`: This provides an API for a large collection of high-level mathematical operations. In addition, it provides support for array operations.\n\n`Math`: This provides access to some mathematical functions also.\n\n`pytest`: This is the way we perform testing on our codes.\n\nAlso, we include Travis CI and CodeCov to make sure that the validility of building and the code. \n  \n* **Future implementation**\n\nPlease refer to section 6. Future Feature.\n\n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00006-ceec5c5b-eb57-4099-b86e-ba70cb1071eb","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 6. Extensions: Reverse Mode & Symbolic Expression\n\n### 6.1 Symbolic Expression (Reverse Diff and Higher Order Derivative) \nFor extension, we built a Expression class to implement the symbolic representation of our function. \n\n#### 6.1.1 Implementation\nExpression class is an abstracted representation of a mathematical function. For instance, some expression represents functions in the form ``a+b``,\nin addition to the arithematic expressions, there are two additional expressions, namly `Symbol` and `Constant`. \n\nEach expression knows how to evaluate itself against a list of given value of each `Symbol`.\n\nEach expression are also implemented to differentiate themselves w.r.t any given `Symbol`, this differentiation returns an expression.\n\nIn this way, as each expression knows how to differentiate itself w.r.t a given `Symbol` and return an expression, which means each expression naturally has the ability of doing higher order function.\n\nTo get value, derivative or jacobian, we just needs to evaluate expression, derivative expression or jacobian expression at their given value.\n\nConceptually, by maintaining a syntax tree, this implementation also meets our definition of `reverse mode differentiation` in the sense that each expression asks its dependencies, combining the results of its dependencies only at the time when `diff()` is called.\n\n\n#### 6.1.2 Functionality\n* Evaluate \n\n* Differentation\n\n* Evaluate at Differentiation Expression\n\n* Higher Order Differentation\n\n\n#### 6.1.3 Usage\n\n\n\n\n### 6.2 Reverse Mode\n\n#### 6.2.1 Explanation and Background\n\nIt should be noticed that in forward mode, chain rule is not utilized. We just follow the evaluation trace and combine the derivatives of elementary functions together. But for reverse mode, we will implement chain rule. \n\nHowever, it is important to realize that the reverse mode also requires the evaluation trace on forward mode to have the derivatives on the elementary functions. Then we will use chain rule to reversely calculate the final derivative.\n\nThe steps we use to implement reverse mode based on the evaluation trace in forward mode is as follows.\n\n- STEP1: Start with $v_1$\n\n   $$\\overline{v_1} = \\dfrac{\\partial f}{\\partial v_1} = 1.$$\n\n- STEP2: Use chain rule to calculate$\\overline{x}$\n\n   $$\\overline{x} = \\dfrac{\\partial f}{\\partial v_1}\\dfrac{\\partial v_1}{\\partial x}  = 1.$$\n\n- STEP3: Get the derivative on x\n\n   $$\\overline{x} = \\dfrac{\\partial f}{\\partial x}  = 1.$$\n\n** Computational graph **\n\nIn reverse mode, we just reversely implement chain rule to get the derivatives. And the computational graph for reverse mode is as follows.\n\n<img src=\"reverse_mode.png\">\n\n*Note: If you cannot see the image, please right click and open image in new tab*\n\n#### 6.2.2 Implementation\nTo implement reverse mode, we create a Node Class to store the nodes/traces in the computational graph, modify elementary functions to customize node objects,\n\nto store the value, input nodes, gradients with respect to the input nodes, and its derivative.\n","metadata":{"tags":[],"cell_id":"00007-efc75aa3-c4f6-4aad-8c72-d075b7eb9db3","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 7. Broader Impact and Inclusivity Statement \n\n### Broader Impact\n\n\n\n### Software Inclusivity\nThis package welcomes and encourage participation and usage from a global community. \nJust as Python Software's Diversity Statement indicated, *the Python community is based on mutual respect, tolerance, and encouragement, and we are working to help each other live up to these principles.* We, as the developer of this AutoDiff package, also want our user group to be more diverse: whoever you are, and whatever your background, we welcome you to use our package.\nThis software package is built based upon the diversity perspective on Python broader community. We strongly believe that embrace diverse community to use our package brings new blood and perspective, making our user group stronger and more vibrant. A diverse user group where all users treat each other with respect has more potential contributors and more sources for fresh ideas.\nWe also welcomes users from all language background. Mathematics has no boundary.\n","metadata":{"tags":[],"cell_id":"00008-fd61fef8-267c-4a0c-bc92-9d9c90e0f5b4","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00009-02e1abdc-6bff-43f5-9d3a-7ffa72357e96","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00007-73f8ce67-dd3a-4fb3-9851-cb4e5e6bee0d","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 8. Reference\n\n[[1]](https://www.jmlr.org/papers/volume18/17-468/17-468.pdf): Baydin, Atilim Gunes; Pearlmutter, Barak; Radul, Alexey Andreyevich; Siskind, Jeffrey (2018). \"Automatic differentiation in machine learning: a survey\". Journal of Machine Learning Research. 18: 1–43.\n\n[[2]](https://fac.ksu.edu.sa/sites/default/files/numerical_analysis_9th.pdf):Rirchard L. Burden and J. Douglas Faires. Numerical Analysis. Brooks/Cole, 2001.\n\n[[3]](https://www.springer.com/gp/book/9783540654667):Johannes Grabmeier and Erich Kaltofen. Computer Algebra Handbook: Foundations, Applications, Systems. Springer, 2003\n\n[[4]](https://www.jstor.org/stable/24103956): Arun Verma. An introduction to automatic differentiation. Current Science, 78(7):804–7,\n2000.\n\n[[5]](https://www.worldcat.org/oclc/31441929):  Spivak, Michael. (1994). *Calculus* (3rd ed.). Houston, Tex.: Publish or Perish. p. 359.\n\n\n","metadata":{"tags":[],"output_cleared":false,"cell_id":"00007-0bbd384f-b3d5-48d3-8505-35b35eaea174","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"output_cleared":false,"cell_id":"00008-842478f6-3816-4bdd-b905-1b35e41b4f90","deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"64662b78-00b4-4022-a0e3-4105838ce0f1","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}}}